{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV tutorial\n",
    "<p>works without needing to install anything except opencv</p>\n",
    "<p>only works with CPU</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi==2020.12.5 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2020.12.5)\n",
      "Requirement already satisfied: numpy==1.19.5 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.19.5)\n",
      "Requirement already satisfied: opencv-python==4.5.1.48 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (4.5.1.48)\n",
      "Requirement already satisfied: wincertstore==0.2 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring parameters(model)\n",
    "<p>weight files: it's the trained model, the core of the algorithm to detect the objects.</p>\n",
    "<p>cfg files: it's the configuration file, where there are all the settings of the algorithm.</p>\n",
    "<p>name files: contains the name of the objects that the algorithm can detect.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Yolo\n",
    "model_path = \"../pre-requisites/\"\n",
    "net = cv2.dnn.readNet(model_path+\"yolov3.weights\", model_path+\"yolov3.cfg\")\n",
    "classes = []\n",
    "with open(model_path+\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the image\n",
    "<p>Load the image where we want to perform the object detection and we also get its width and height</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading image\n",
    "# img = cv2.imread(\"room_ser.jpg\")\n",
    "# img = cv2.imread(\"drmaemi_room.jpg\")\n",
    "img = cv2.imread(\"images/input/drmaemi_living_room.jpg\")\n",
    "img = cv2.resize(img, None, fx=0.4, fy=0.4)\n",
    "height, width, channels = img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass the img into the network and do the detection\n",
    "<p>We can't use right away the full image on the network.</p>\n",
    "<p>First, we need it to convert it to blob, used to extract features from the image and to resize them.</p>\n",
    "<p>\n",
    "    \n",
    "YOLO accepts three sizes:\n",
    "- 320 x 320 : it's small so less accuracy but better speed\n",
    "- 416 x 416 : it's in the middle and you get a bit of both.\n",
    "- 609 x 609 : it's bigger so high accuracy and slow speed\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting objects\n",
    "blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "net.setInput(blob)\n",
    "outs = net.forward(output_layers) # the result of the detection\n",
    "# 'outs' is an array that contains all the info about objects detected, their position and the confidence about the detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection done\n",
    "<p>At this point the detection is done.</p>\n",
    "<p>We only need to show the result on the screen.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the outs array, we calculate the confidence and we choose a confidence threshold.\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            # Object detected\n",
    "            # Pre-process because original coordinates in detected box is regularized between 0 to 1\n",
    "            center_x = int(detection[0]*width)\n",
    "            center_y = int(detection[1]*height)\n",
    "            w = int(detection[2]*width)\n",
    "            h = int(detection[3]*height)\n",
    "            \n",
    "            # Rectangle coordinates\n",
    "            x = int(center_x - w/2)\n",
    "            y = int(center_y - h/2)\n",
    "            \n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to use on maximum suppresion\n",
    "<p>When we perform the detection, it happens that we have more boxes for the same object.</p>\n",
    "<p>So we should use another function to remove this 'noise'.</p>\n",
    "<p>\n",
    "    \n",
    "It's called **Non maximum suppresion**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally extract all the info & show them on the screen.\n",
    "- Box : contain the coordinates of the rectangle surrounding the object detected.\n",
    "- Label : name of the object detected\n",
    "- Confidence : the confidence about the detection from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(classes[class_ids[i]])\n",
    "        color = colors[i]\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)\n",
    "        cv2.putText(img, label, (x, y+30), font, 3, color, 3)\n",
    "\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.imwrite('images/output/drmaemi_living_room.png', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenCVenv",
   "language": "python",
   "name": "opencvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
